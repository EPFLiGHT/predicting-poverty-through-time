{"cells":[{"cell_type":"markdown","metadata":{"id":"FdiEWCD74ufa"},"source":["# Process Surveys\n","We need to process the raw data, so we can use it to scrape images and as a base for our models. From the LSMS surveys we need two files - the one which contains the geovariables (lat and lon of the cluster) and one which contains the consumption. Sometimes it is a bit tricky to get the data, since they are linked through some keys which lays in other files."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20254,"status":"ok","timestamp":1671117160377,"user":{"displayName":"Poverty Map Mle","userId":"07895896395619789035"},"user_tz":-60},"id":"vSLKHxDO41MV","outputId":"c522ad29-9572-42de-c106-b37d9b75286a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount the drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8587,"status":"ok","timestamp":1671117168961,"user":{"displayName":"Poverty Map Mle","userId":"07895896395619789035"},"user_tz":-60},"id":"QkrMbqq_-1_H","outputId":"2ce7fd42-3b2f-4c0f-fa4e-1ce927e11010"},"outputs":[],"source":["# Install WorlBank api\n","!pip install world-bank-data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671117168962,"user":{"displayName":"Poverty Map Mle","userId":"07895896395619789035"},"user_tz":-60},"id":"rSHPE2-0otSu","outputId":"d5a299dc-e898-4be5-fe42-7286b4b98b2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/src\n"]}],"source":["cd /content/drive/MyDrive/src"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYYzVVn44uff"},"outputs":[],"source":["from lib.lsms import LSMS\n","from tqdm import tqdm\n","import json\n","import os\n","import pandas as pd\n","\n","from typing import List, Set, Dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_ym8ldI5Mej"},"outputs":[],"source":["# Select the required country (only adapted to africa so far)\n","continent: str = 'africa'"]},{"cell_type":"markdown","metadata":{"id":"94crKV_8IVp8"},"source":["The LSMS surveys are downloaded as zip files, to avoid the extraction manually we prepared this function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIupmpIyg2u-"},"outputs":[],"source":["import zipfile\n","\n","def unzip(path: str) -> None:\n","    \"\"\"Function to unzip all zipped LSMS surveys. \n","\n","    Args:\n","        path (str): directory where to find the countries\n","        \n","    \"\"\"\n","    os.chdir(path)\n","    dir = path + 'raw/'\n","\n","    for country in os.listdir(dir): # Iterate through all countries\n","      for year in os.listdir(f\"{dir}{country}/\"): #I terate through all years\n","        for item in os.listdir(f\"{dir}{country}/{year}/\"): # Iterate through all items in the folder\n","          if item.endswith('.zip') or item.endswith('.ZIP'):\n","            zip_ref = zipfile.ZipFile(f\"{dir}{country}/{year}/{item}\") # create zipfile object\n","            zip_ref.extractall(f\"{dir}{country}/{year}/{item[:-4]}\") # extract file to dir\n","            zip_ref.close() # close file\n","            os.remove(f\"{dir}{country}/{year}/{item}\") # delete zipped file\n","            break\n","\n","    # Reset the cd\n","    os.chdir('/content/drive/MyDrive/src')\n","\n","# Unzip all the zip files in the folder\n","dir: str = f'/content/drive/MyDrive/data/continents/{continent}/lsms/'\n","unzip(dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvVi07Py4uff"},"outputs":[],"source":["def tza(data: any, year: str, ppp: int) -> pd.DataFrame:\n","    \"\"\"Function to process the Tanzania Survey of 2015. The link between the coordinates of the clusters is done through `hh_sec_a.csv`.\n","\n","    Args:\n","        data (any): json entry of the current data\n","        year (str): current year (should be 2015 in the gives case)\n","        ppp (int): ppp, for nominal and real cons\n","\n","    Returns:\n","        pd.DataFrame: processed dataframe\n","\n","    \"\"\"\n","    df: pd.DataFrame = pd.read_csv(f\"../{data['cluster_path']}\")\n","    df_hh: pd.DataFrame = pd.read_csv(f\"../{data['hh_path']}\")\n","    tmp: pd.DataFrame = df_hh.merge(df, on=[\"clusterid\"])\n","    name: str = \"TZA-Areallyrandomfile39493208943.csv\"  # tmp file\n","    tmp.to_csv(name)\n","    lsms: LSMS = LSMS(\n","        \"TZA\", year, cons_path=f\"../{data['cons_path']}\", hh_path=name)\n","    lsms.read_data()\n","    lsms.process_survey(cons_key=data[\"cons_key\"], hhsize_key=data[\"hhsize_key\"], lat_key=data[\"lat_key\"], lon_key=data[\"lon_key\"],\n","                        hhid_key=data[\"hhid_key\"], multiply=data[\"multiply\"], rural_key=data[\"rural_key\"], rural_tag=data[\"rural\"], urban_tag=data[\"urban\"])\n","    lsms.write_processed(f\"../data/continents/{continent}/lsms/processed/TZA_{year}.csv\")\n","    os.remove(name) # delete tmp file\n","    \n","    return lsms.processed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_K5LV0jnOGl"},"outputs":[],"source":["from geopy.geocoders import Nominatim\n","def uga(path: str) -> None:\n","    \"\"\"Function to process the Uganda Survey of 2016. \n","    The link between the coordinates of the districts and the districts themselves is done through geopy.\n","\n","    Args:\n","        path (str): path to Uganda 2016 folder\n","\n","    \"\"\"\n","\n","    # Creating a df to be used by the API\n","    with open(path + \"districts.json\", \"r\") as f:\n","        map_ = json.load(f)\n","    df: pd.DataFrame = pd.DataFrame.from_dict(map_, orient='index')\n","    df.columns = ['district']\n","    df.district = df.district.apply(lambda di: \"Uganda, \"+di)\n","    df = df.dropna()\n","\n","    # Instanciating geolocator:\n","    geolocator: any = Nominatim(user_agent=\"https\")\n","\n","    # Add columns for latitude and longitude\n","    df['Latitude'] = df['district'].apply(lambda x: geolocator.geocode(x).latitude)\n","    df['Longitude'] = df['district'].apply(lambda x: geolocator.geocode(x).longitude)\n","\n","    # Adding manually two corner cases not solved by the API (source Wikipedia)\n","    df.loc[\"109\"] = {'district': 'Uganda, Nakasangola', 'Latitude': 1.3150, 'Longitude' : 32.4650 }\n","    df.loc[\"411\"] = {'district': 'Uganda, Mbarara', 'Latitude': -0.8758, 'Longitude': 30.2592}\n","\n","    # Renaming columns\n","    df = (df.reset_index)()\n","    df = df.drop(['district'], axis=1).rename(columns = {'index': 'district'})\n","\n","    # Loading Uganda data\n","    uganda_2016: pd.DataFrame = pd.read_csv(path + \"UGA_2015_UNPS_v02_M_CSV/pov2015_16.csv\") \n","\n","    # Merging df with uganda data and renaming the columns so that match JSON file keys' fields\n","    uganda_2016 = uganda_2016.merge(df, on='district')\n","    uganda_2016 = uganda_2016.rename(columns={'Latitude': 'lat', 'Longitude': 'long'})\n","\n","    # Save the new dataframe as csv\n","    uganda_2016.to_csv(path + \"UGA_2015_UNPS_v02_M_CSV/pov_mod2015_16.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"juWrfOC14ufg"},"source":["Loads json file which contains the rules for processing. Have a look in the Readme.md in the `data/LSMS` folder to understand the structure of the file. It can be extended easily."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A09Zg7ui4ufg"},"outputs":[],"source":["with open(f\"../data/continents/{continent}/lsms/country_keys.json\", \"r\") as f:\n","    data = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8606,"status":"ok","timestamp":1671117180232,"user":{"displayName":"Poverty Map Mle","userId":"07895896395619789035"},"user_tz":-60},"id":"84RZMQtwd-La","outputId":"78657440-a1c9-43fa-aab1-6eea74242e04"},"outputs":[],"source":["!pip install pyreadstat"]},{"cell_type":"markdown","metadata":{"id":"vzuoKjEy4ufh"},"source":["It's convenient to have one large file with all countries included. So we will also save it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KILSXF1U4ufh"},"outputs":[],"source":["def run(path: str, nominal: bool = True) -> None:\n","    \"\"\"\n","    Helper function to get LSMS. \n","\n","    Args:\n","        path (str): Path to write complete dataframe\n","        nominal (bool): Set mode for nominal or real consumption \n","    \n","    \"\"\"\n","\n","    if nominal:\n","        ppp: int = 1\n","    else:\n","        ppp: int = -1\n","\n","    master_df: pd.DataFrame = pd.DataFrame()\n","\n","    for country in data:\n","        for year in data[country]:\n","            cur = data[country][year]\n","            if cur[\"special\"]:\n","                if country == \"TZA\" and year == \"2014\":\n","                    tmp_df: pd.DataFrame = tza(cur, year, ppp)\n","                    master_df = pd.concat([master_df,tmp_df])\n","                    continue\n","                if country == \"UGA\" and year == \"2016\":\n","                    uga(f\"../data/continents/{continent}/lsms/raw/Uganda/2016/\")\n","\n","            lsms = LSMS(country, year, cons_path=f\"../{cur['cons_path']}\", hh_path=f\"../{cur['hh_path']}\", ppp=ppp)\n","            lsms.read_data()\n","            lsms.process_survey(cons_key=cur[\"cons_key\"], hhsize_key=cur[\"hhsize_key\"], lat_key=cur[\"lat_key\"], lon_key=cur[\"lon_key\"], hhid_key=cur[\"hhid_key\"], rural_key=cur[\"rural_key\"], rural_tag=cur[\"rural\"], urban_tag=cur[\"urban\"],multiply=cur[\"multiply\"])\n","            \n","            if nominal:\n","                ending: str = \"nominal\"\n","            else:\n","                ending: str = \"real\" \n","                \n","            lsms.write_processed(f\"../data/continents/{continent}/lsms/processed/{country}_{year}_{ending}.csv\")\n","            master_df = pd.concat([master_df,lsms.processed])\n","\n","    # Save to csv\n","    master_df.to_csv(path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ssnmR_q4ufi"},"outputs":[],"source":["run(f\"../data/continents/{continent}/lsms/processed/_all_nominal.csv\")\n","run(f\"../data/continents/{continent}/lsms/processed/_all_real.csv\", False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5e6443d47ccd972d9a6180e1150200cb65468c61498eff16b63ba4ff0131abbb"}}},"nbformat":4,"nbformat_minor":0}
