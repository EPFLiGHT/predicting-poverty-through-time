{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Surveys\n",
    "We need to process the raw data, so we can use it to scrape images and as a base for our models. From the LSMS surveys we need two files - the one which contains the geovariables (lat and lon of the cluster) and one which contains the consumption. Sometimes it is a bit tricky to get the data, since they are linked through some keys which lays in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\Desktop\\Master\\MA1\\ML\\ML4Science\\forked-cm110-poverty\\src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.lsms import LSMS\n",
    "from bidict import  bidict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(\"../data/lsms/country_keys.json\", \"r\") as f:\n",
    "    metadata = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads json file which contains the rules for processing. Have a look in the Readme.md in the `data/LSMS` folder to understand the structure of the file. It can be extended easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's convenient to have one large file with all countries included. So we will also save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def process_set_countries(metadata: dict, countries: set[str], nominal: bool):\n",
    "    \"\"\"\n",
    "    Process a set of countries of present years in their necessary metadata and outputs aggregates data\n",
    "    processing could be done in nominal and real way\n",
    "\n",
    "    Args:\n",
    "        metadata:\n",
    "        countries:\n",
    "        nominal:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    ppp = 1 if nominal else -1\n",
    "    master_df: pd.DataFrame = pd.DataFrame()\n",
    "    for country in tqdm(countries):\n",
    "        for year in metadata[country]:\n",
    "\n",
    "            # Ignore special\n",
    "            if metadata[country][year]['special']:\n",
    "                print(f'{country}:{year} is special and was ignored')\n",
    "                continue\n",
    "\n",
    "            cur = metadata[country][year]\n",
    "\n",
    "            lsms = LSMS(country, year, cons_path=f\"../{cur['cons_path']}\", hh_path=f\"../{cur['hh_path']}\", ppp=ppp)\n",
    "            lsms.read_data()\n",
    "            lsms.process_survey(cons_key=cur[\"cons_key\"], hhsize_key=cur[\"hhsize_key\"], lat_key=cur[\"lat_key\"],\n",
    "                                lon_key=cur[\"lon_key\"], hhid_key=cur[\"hhid_key\"], rural_key=cur[\"rural_key\"],\n",
    "                                rural_tag=cur[\"rural\"], urban_tag=cur[\"urban\"], multiply=cur[\"multiply\"])\n",
    "            lsms.write_processed(f\"../data/lsms/processed/{country}_{year}_{'nominal' if nominal else 'real'}.csv\")\n",
    "            master_df = pd.concat([master_df, lsms.processed])\n",
    "    return master_df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from lib.process_uganda import process_uga_2009, process_uga_2010\n",
    "from lib.process_tanzania import process_tza\n",
    "\n",
    "def process_all(output: str, metadata: dict):\n",
    "\n",
    "    countries = {'NER', 'ETH', 'MW', 'MLI', 'NG', 'TZA'}\n",
    "\n",
    "    print(\"Processing countries \")\n",
    "    df_countries_nominal = process_set_countries(metadata=metadata,\n",
    "                                                countries=countries, nominal=True)\n",
    "    df_countries_real = process_set_countries(metadata=metadata,\n",
    "                                             countries=countries, nominal=False)\n",
    "\n",
    "    print(\"Processing Tanzania 2014..\")\n",
    "    df_tza_nominal = process_tza(metadata['TZA']['2014'], '2014', ppp=1)\n",
    "    df_tza_real = process_tza(metadata['TZA']['2014'], '2014', ppp=-1)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Processing Uganda 2009..\")\n",
    "    matched_keys = metadata[\"UGA\"][\"2009\"]\n",
    "    df_uga_2009_nominal, df_uga_2009_real = process_uga_2009(matched_keys)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "    print(\"Processing Uganda 2010..\")\n",
    "    matched_keys = metadata[\"UGA\"][\"2010\"]\n",
    "    df_uga_2010_nominal, df_uga_2010_real = process_uga_2010(matched_keys)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Collecting all data..\")\n",
    "    df_all_nominal = pd.concat([df_countries_nominal, df_tza_nominal, df_uga_2009_nominal, df_uga_2010_nominal])\n",
    "    df_all_real = pd.concat([df_countries_real, df_tza_real, df_uga_2009_real, df_uga_2010_real])\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Drop rows with na values\n",
    "    df_all_real.dropna(inplace=True)\n",
    "    df_all_nominal.dropna(inplace=True)\n",
    "\n",
    "    print(\"Writing to files..\")\n",
    "    df_all_nominal.to_csv(f'{output}/_all_nominal.csv')\n",
    "    df_all_real.to_csv(f'{output}/_all_real.csv')\n",
    "    print(\"Done.\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing countries \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:03<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TZA:2014 is special and was ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TZA:2014 is special and was ignored\n",
      "Processing Tanzania 2014..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Processing Uganda 2009..\n",
      "Done.\n",
      "Processing Uganda 2010..\n",
      "Done.\n",
      "Collecting all data..\n",
      "Done.\n",
      "Writing to files..\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "process_all('../data/lsms/processed', metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
