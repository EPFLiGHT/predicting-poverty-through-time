{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Streetmap Features\n",
    "\n",
    "In this notebook the features from Open Streetmap (OSM) will be extracted. The [ohsome API](https://docs.ohsome.org/ohsome-api/v1/) will be used. It allows us to extract the data at a specific time. A great upside is, that we don't need to process locally which can be a pain for OSM data. Can take up to 2 hours to process.\n",
    "\n",
    "The following features will be extracted:\n",
    "- Buildings\n",
    "- Amenities\n",
    "- Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ohsome import OhsomeClient\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OhsomeClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the buffer around the our cluster. Our cluster is 6.74km x 6.74km. Now we need to create also an area of this size. The buffer need to be the half of the side, since it the radius. We do the conversation to not manually calculate the buffer (which is not trivial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(\"../../data/lsms/processed/_all_nominal.csv\")\n",
    "gdf[\"geometry\"] =  gpd.points_from_xy(gdf.lon, gdf.lat)\n",
    "gdf.crs = 4326\n",
    "gdf = gdf.to_crs(3857)\n",
    "gdf[\"buffer\"]= gdf.geometry.buffer(3360, cap_style=3).to_crs(4326)\n",
    "gdf = gdf.to_crs(4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_building = \"building=*\"\n",
    "filter_residential = \"residential=* or building=residential or abutters=residential or construction=residential or landuse=residential\"\n",
    "filter_industry = \"industry=*\"\n",
    "filter_commercial = \"landuse=commercial or building=commercial or building=office\"\n",
    "filter_education = \"amenity=school or amenity=kindergarten or amenity=university or amenity=college or landuse=education\"\n",
    "filter_health = \"healthcare=* or amenity=doctors or amenity=hospital or amenity=pharmacy\"\n",
    "filter_buildings = [filter_building, filter_residential,\n",
    "                    filter_industry, filter_education, filter_health]\n",
    "keys_building = [\"building\", \"residential\", \"industry\", \"education\", \"health\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_func = client.elements.count.groupByBoundary\n",
    "area_func = client.elements.area.groupByBoundary\n",
    "area_dens_func = client.elements.area.density.groupByBoundary\n",
    "funcs = [count_func, area_func, area_dens_func]\n",
    "keys_func = [\"count\", \"area\", \"density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(resp, value_name):\n",
    "    ids = []\n",
    "    value = resp.value.to_list()\n",
    "    for cluster in resp.value.keys():\n",
    "        ids.append(cluster[0])\n",
    "    return pd.DataFrame.from_dict({\"id\": ids, value_name: value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_buildings(bboxes, year, country):\n",
    "    df_full = pd.DataFrame()\n",
    "    for key, filter in zip(keys_building, filter_buildings):\n",
    "        for func_key, func in zip(keys_func, funcs):\n",
    "            response = func.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=filter).as_dataframe()\n",
    "            processed_response = process_response(response, f\"{key}_{func_key}\")\n",
    "        \n",
    "            if len(df_full) == 0:\n",
    "                df_full = df_full.append(processed_response)\n",
    "            else:\n",
    "                df_full = df_full.merge(processed_response, on=\"id\")\n",
    "    df_full.to_csv(f\"../../data/osm_features/{country}_{year}_buildings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country ETH_2013 already exists in osm\n",
      "Country ETH_2015 already exists in osm\n",
      "Country ETH_2018 already exists in osm\n",
      "Country MLI_2014 already exists in osm\n",
      "Country MLI_2018 already exists in osm\n",
      "Country MW_2016 already exists in osm\n",
      "Country MW_2019 already exists in osm\n",
      "New country NER_2011 to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:49<00:49,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country NER_2018 already exists in osm\n",
      "Country NG_2012 already exists in osm\n",
      "Country NG_2015 already exists in osm\n",
      "Country NG_2018 already exists in osm\n",
      "Country TZA_2012 already exists in osm\n",
      "Country TZA_2014 already exists in osm\n",
      "New country UGA_2009 to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [02:02<00:08,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New country UGA_2010 to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [03:12<00:00, 12.05s/it]\n"
     ]
    }
   ],
   "source": [
    "surveys = gdf.groupby([\"country\", \"year\"]).groups.keys()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    if os.path.exists(f\"../../data/osm_features/{country}_{year}_buildings.csv\"):\n",
    "        print(f'Country {country}_{year} already exists in osm')\n",
    "        continue\n",
    "\n",
    "    print(f'New country {country}_{year} to process')\n",
    "    subset_df = gdf[(gdf['country'] == country) & (gdf['year'] == year)].reset_index(drop=True)\n",
    "    bboxes = {}\n",
    "    for _, item in subset_df.iterrows():\n",
    "        ymin, xmix, xmax, ymax = item.buffer.bounds\n",
    "        bboxes[item.id] = [ymin, xmix, xmax, ymax]\n",
    "    \n",
    "    extract_buildings(bboxes, year, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 139.12it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    tmp_df = pd.read_csv(f\"../../data/osm_features/{country}_{year}_buildings.csv\")\n",
    "    total_df = total_df.append(tmp_df)\n",
    "total_df.to_csv(\"../../data/osm_features/_all_buildings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POI Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = {'monument', 'kindergarten', 'town_hall', 'stadium', 'optician', 'post_box', 'laundry', 'playground', 'computer_shop', 'outdoor_shop', 'florist', 'prison', 'atm', 'mall', 'camp_site', 'gift_shop', 'community_centre', 'veterinary', 'greengrocer', 'bar', 'sports_centre', 'university', 'jeweller', 'bank', 'mobile_phone_shop', 'camera_surveillance', 'drinking_water', 'pitch', 'track', 'toilet', 'water_tower', 'chalet', 'car_rental', 'dentist', 'furniture_shop', 'artwork', 'beauty_shop', 'library', 'tourist_info', 'park', 'viewpoint', 'motel', 'graveyard', 'hospital', 'comms_tower', 'shelter', 'hostel', 'beverages', 'public_building', 'museum', 'swimming_pool', 'kiosk', 'college', 'hairdresser', 'attraction', 'water_well', 'bookshop', 'recycling', 'pharmacy', 'sports_shop', 'cafe', 'theatre', 'guesthouse', 'stationery', 'picnic_site', 'clothes', 'pub', 'hotel', 'nightclub', 'fire_station', 'cinema', 'restaurant', 'waste_basket', 'shoe_shop', 'bicycle_shop', 'police', 'school', 'butcher', 'doityourself', 'chemist', 'car_wash', 'telephone', 'car_dealership', 'toy_shop', 'fast_food', 'food_court', 'tower', 'bakery', 'memorial', 'others', 'supermarket', 'post_office', 'courthouse', 'doctors', 'convenience', 'embassy', 'bench', 'department_store', 'travel_agent', 'fountain', 'water_works'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pois(bboxes, year, country):\n",
    "    resp_pois = client.elements.count.groupByBoundary.groupByTag.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"amenity=*\", groupByKey=\"amenity\").as_dataframe()\n",
    "    cur_key = resp_pois.value.keys()[0][0]\n",
    "    pois_dic = {}\n",
    "    for poi in pois:\n",
    "        pois_dic[poi] = []\n",
    "    cur_set = pois.copy()\n",
    "    values = resp_pois.value.to_list()\n",
    "    pois_dic[\"id\"] = [cur_key]\n",
    "    for i, cur in enumerate(resp_pois.value.keys()):\n",
    "        cur_poi = cur[1].replace(\"amenity=\", \"\").replace(\" \", \"_\").lower()\n",
    "        if cur_poi in cur_set:\n",
    "            pois_dic[cur_poi].append(values[i])\n",
    "            cur_set.remove(cur_poi)\n",
    "\n",
    "        if cur[0] != cur_key:\n",
    "            cur_key = cur[0]\n",
    "            pois_dic[\"id\"].append(cur_key)\n",
    "            for missing in cur_set:\n",
    "                pois_dic[missing].append(0)\n",
    "            cur_set = pois.copy()\n",
    "            \n",
    "    for missing in cur_set:\n",
    "        pois_dic[missing].append(0)\n",
    "\n",
    "    pd.DataFrame.from_dict(pois_dic).to_csv(f\"../../data/osm_features/{country}_{year}_pois.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 9945.00it/s]\n"
     ]
    }
   ],
   "source": [
    "surveys = gdf.groupby([\"country\", \"year\"]).groups.keys()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    if os.path.exists(f\"../../data/osm_features/{country}_{year}_pois.csv\"):\n",
    "        continue\n",
    "    subset_df = gdf[(gdf['country'] == country) & (gdf['year'] == year)].reset_index(drop=True)\n",
    "    bboxes = {}\n",
    "    for _, item in subset_df.iterrows():\n",
    "        ymin, xmix, xmax, ymax = item.buffer.bounds\n",
    "        bboxes[item.id] = [ymin, xmix, xmax, ymax]\n",
    "    \n",
    "    get_pois(bboxes, year, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 41.14it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    tmp_df = pd.read_csv(f\"../../data/osm_features/{country}_{year}_pois.csv\")\n",
    "    total_df = total_df.append(tmp_df)\n",
    "total_df.to_csv(\"../../data/osm_features/_all_pois.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roads = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_filters = {\"residential\", \"track\", \"living_street\", \"trunk\", \"primary\", \"secondary\", \"tertiary\", \"service\", \"pedestrian\", \"intersection\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_func = client.elements.count.groupByBoundary.groupByTag\n",
    "len_func = client.elements.length.groupByBoundary.groupByTag\n",
    "dens_func = client.elements.length.density.groupByBoundary.groupByTag\n",
    "road_funcs = [count_func, len_func, dens_func]\n",
    "road_func_keys = [\"count\", \"length\", \"density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_road_features(bboxes, year, country):\n",
    "    df_roads = pd.DataFrame()\n",
    "\n",
    "    response_count = client.elements.count.groupByBoundary.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\")\n",
    "    tota_count = process_response(response_count.as_dataframe(), \"total_count\")\n",
    "    df_roads = df_roads.append(tota_count)\n",
    "\n",
    "    response_len = client.elements.length.groupByBoundary.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\")\n",
    "    tota_len = process_response(response_len.as_dataframe(), \"total_length\")\n",
    "    df_roads = df_roads.merge(tota_len, on=\"id\")\n",
    "\n",
    "    response_dens = client.elements.length.density.groupByBoundary.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\")\n",
    "    tota_dens = process_response(response_dens.as_dataframe(), \"total_density\")\n",
    "    df_roads = df_roads.merge(tota_dens, on=\"id\")\n",
    "\n",
    "    road_dic = {}\n",
    "    for poi in road_filters:\n",
    "        road_dic[poi] = []\n",
    "\n",
    "    for key, func in zip(road_func_keys, road_funcs):\n",
    "        resp_road = func.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\", groupByKey=\"highway\").as_dataframe()\n",
    "        road_dic = {}\n",
    "        for poi in road_filters:\n",
    "            road_dic[f\"{key}_{poi}\"] = []\n",
    "\n",
    "        cur_key = resp_road.value.keys()[0][0]\n",
    "        cur_set = road_filters.copy()\n",
    "        values = resp_road.value.to_list()\n",
    "        road_dic[\"id\"] = [cur_key]\n",
    "        for i, cur in enumerate(resp_road.value.keys()):\n",
    "            # print(cur)\n",
    "            cur_poi = cur[1].replace(\"highway=\", \"\").replace(\" \", \"_\").lower()\n",
    "            if cur_poi in cur_set:\n",
    "                road_dic[f\"{key}_{cur_poi}\"].append(values[i])\n",
    "                cur_set.remove(cur_poi)\n",
    "\n",
    "            if cur[0] != cur_key:\n",
    "                cur_key = cur[0]\n",
    "                road_dic[\"id\"].append(cur_key)\n",
    "                for missing in cur_set:\n",
    "                    road_dic[f\"{key}_{missing}\"].append(0)\n",
    "                cur_set = road_filters.copy()\n",
    "                \n",
    "        for missing in cur_set:\n",
    "            road_dic[f\"{key}_{missing}\"].append(0)\n",
    "\n",
    "        tmp_dic = pd.DataFrame.from_dict(road_dic)\n",
    "        df_roads = df_roads.merge(tmp_dic, on=\"id\")\n",
    "    df_roads.to_csv(f\"../../data/osm_features/{country}_{year}_road.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:54<00:00,  3.42s/it]\n"
     ]
    }
   ],
   "source": [
    "surveys = gdf.groupby([\"country\", \"year\"]).groups.keys()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    if os.path.exists(f\"../../data/osm_features/{country}_{year}_road.csv\"):\n",
    "        continue\n",
    "    subset_df = gdf[(gdf['country'] == country) & (gdf['year'] == year)].reset_index(drop=True)\n",
    "    bboxes = {}\n",
    "    for _, item in subset_df.iterrows():\n",
    "        ymin, xmix, xmax, ymax = item.buffer.bounds\n",
    "        bboxes[item.id] = [ymin, xmix, xmax, ymax]\n",
    "    \n",
    "    extract_road_features(bboxes, year, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 84.93it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    tmp_df = pd.read_csv(f\"../../data/osm_features/{country}_{year}_road.csv\")\n",
    "    total_df = total_df.append(tmp_df)\n",
    "total_df.to_csv(\"../../data/osm_features/_all_road.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
