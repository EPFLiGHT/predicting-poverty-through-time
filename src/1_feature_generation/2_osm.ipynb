{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Streetmap Features\n",
    "\n",
    "In this notebook the features from Open Streetmap (OSM) will be extracted. The [ohsome API](https://docs.ohsome.org/ohsome-api/v1/) will be used. It allows us to extract the data at a specific time. A great upside is, that we don't need to process locally which can be a pain for OSM data. Can take up to 2 hours to process.\n",
    "\n",
    "The following features will be extracted:\n",
    "- Buildings\n",
    "- Amenities\n",
    "- Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ohsome import OhsomeClient\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OhsomeClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create the buffer around the our cluster. Our cluster is 6.74km x 6.74km. Now we need to create also an area of this size. The buffer need to be the half of the side, since it the radius. We do the conversation to not manually calculate the buffer (which is not trivial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/geopandas/array.py:275: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  return GeometryArray(vectorized.points_from_xy(x, y, z), crs=crs)\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file(\"../../data/lsms/processed/_all_nominal.csv\")\n",
    "gdf[\"geometry\"] =  gpd.points_from_xy(gdf.lon, gdf.lat)\n",
    "gdf.crs = 4326\n",
    "gdf = gdf.to_crs(3857)\n",
    "gdf[\"buffer\"]= gdf.geometry.buffer(3360, cap_style=3).to_crs(4326)\n",
    "gdf = gdf.to_crs(4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_building = \"building=*\"\n",
    "filter_residential = \"residential=* or building=residential or abutters=residential or construction=residential or landuse=residential\"\n",
    "filter_industry = \"industry=*\"\n",
    "filter_commercial = \"landuse=commercial or building=commercial or building=office\"\n",
    "filter_education = \"amenity=school or amenity=kindergarten or amenity=university or amenity=college or landuse=education\"\n",
    "filter_health = \"healthcare=* or amenity=doctors or amenity=hospital or amenity=pharmacy\"\n",
    "filter_buildings = [filter_building, filter_residential,\n",
    "                    filter_industry, filter_education, filter_health]\n",
    "keys_building = [\"building\", \"residential\", \"industry\", \"education\", \"health\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_func = client.elements.count.groupByBoundary\n",
    "area_func = client.elements.area.groupByBoundary\n",
    "area_dens_func = client.elements.area.density.groupByBoundary\n",
    "funcs = [count_func, area_func, area_dens_func]\n",
    "keys_func = [\"count\", \"area\", \"density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(resp, value_name):\n",
    "    ids = []\n",
    "    value = resp.value.to_list()\n",
    "    for cluster in resp.value.keys():\n",
    "        ids.append(cluster[0])\n",
    "    return pd.DataFrame.from_dict({\"id\": ids, value_name: value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_buildings(bboxes, year, country):\n",
    "    df_full = pd.DataFrame()\n",
    "    for key, filter in zip(keys_building, filter_buildings):\n",
    "        for func_key, func in zip(keys_func, funcs):\n",
    "            response = func.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=filter).as_dataframe()\n",
    "            processed_response = process_response(response, f\"{key}_{func_key}\")\n",
    "        \n",
    "            if len(df_full) == 0:\n",
    "                df_full = df_full.append(processed_response)\n",
    "            else:\n",
    "                df_full = df_full.merge(processed_response, on=\"id\")\n",
    "    df_full.to_csv(f\"../../data/osm_features/{country}_{year}_buildings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys = gdf.groupby([\"country\", \"year\"]).groups.keys()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    if os.path.exists(f\"../../data/osm_features/{country}_{year}_buildings.csv\"):\n",
    "        continue\n",
    "    subset_df = gdf[(gdf['country'] == country) & (gdf['year'] == year)].reset_index(drop=True)\n",
    "    bboxes = {}\n",
    "    for _, item in subset_df.iterrows():\n",
    "        ymin, xmix, xmax, ymax = item.buffer.bounds\n",
    "        bboxes[item.id] = [ymin, xmix, xmax, ymax]\n",
    "    \n",
    "    extract_buildings(bboxes, year, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 428.58it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    tmp_df = pd.read_csv(f\"../../data/osm_features/{country}_{year}_buildings.csv\")\n",
    "    total_df = total_df.append(tmp_df)\n",
    "total_df.to_csv(\"../../data/osm_features/_all_buildings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POI Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = {'monument', 'kindergarten', 'town_hall', 'stadium', 'optician', 'post_box', 'laundry', 'playground', 'computer_shop', 'outdoor_shop', 'florist', 'prison', 'atm', 'mall', 'camp_site', 'gift_shop', 'community_centre', 'veterinary', 'greengrocer', 'bar', 'sports_centre', 'university', 'jeweller', 'bank', 'mobile_phone_shop', 'camera_surveillance', 'drinking_water', 'pitch', 'track', 'toilet', 'water_tower', 'chalet', 'car_rental', 'dentist', 'furniture_shop', 'artwork', 'beauty_shop', 'library', 'tourist_info', 'park', 'viewpoint', 'motel', 'graveyard', 'hospital', 'comms_tower', 'shelter', 'hostel', 'beverages', 'public_building', 'museum', 'swimming_pool', 'kiosk', 'college', 'hairdresser', 'attraction', 'water_well', 'bookshop', 'recycling', 'pharmacy', 'sports_shop', 'cafe', 'theatre', 'guesthouse', 'stationery', 'picnic_site', 'clothes', 'pub', 'hotel', 'nightclub', 'fire_station', 'cinema', 'restaurant', 'waste_basket', 'shoe_shop', 'bicycle_shop', 'police', 'school', 'butcher', 'doityourself', 'chemist', 'car_wash', 'telephone', 'car_dealership', 'toy_shop', 'fast_food', 'food_court', 'tower', 'bakery', 'memorial', 'others', 'supermarket', 'post_office', 'courthouse', 'doctors', 'convenience', 'embassy', 'bench', 'department_store', 'travel_agent', 'fountain', 'water_works'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pois(bboxes, year, country):\n",
    "    resp_pois = client.elements.count.groupByBoundary.groupByTag.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"amenity=*\", groupByKey=\"amenity\").as_dataframe()\n",
    "    cur_key = resp_pois.value.keys()[0][0]\n",
    "    pois_dic = {}\n",
    "    for poi in pois:\n",
    "        pois_dic[poi] = []\n",
    "    cur_set = pois.copy()\n",
    "    values = resp_pois.value.to_list()\n",
    "    pois_dic[\"id\"] = [cur_key]\n",
    "    for i, cur in enumerate(resp_pois.value.keys()):\n",
    "        cur_poi = cur[1].replace(\"amenity=\", \"\").replace(\" \", \"_\").lower()\n",
    "        if cur_poi in cur_set:\n",
    "            pois_dic[cur_poi].append(values[i])\n",
    "            cur_set.remove(cur_poi)\n",
    "\n",
    "        if cur[0] != cur_key:\n",
    "            cur_key = cur[0]\n",
    "            pois_dic[\"id\"].append(cur_key)\n",
    "            for missing in cur_set:\n",
    "                pois_dic[missing].append(0)\n",
    "            cur_set = pois.copy()\n",
    "            \n",
    "    for missing in cur_set:\n",
    "        pois_dic[missing].append(0)\n",
    "\n",
    "    pd.DataFrame.from_dict(pois_dic).to_csv(f\"../../data/osm_features/{country}_{year}_pois.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 32363.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETH 2013\n",
      "ETH 2015\n",
      "ETH 2018\n",
      "MW 2016\n",
      "MW 2019\n",
      "NG 2012\n",
      "NG 2015\n",
      "NG 2018\n",
      "TZA 2012\n",
      "TZA 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "surveys = gdf.groupby([\"country\", \"year\"]).groups.keys()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    print(country, year)\n",
    "    if os.path.exists(f\"../../data/osm_features/{country}_{year}_pois.csv\"):\n",
    "        continue\n",
    "    # print(f\"Start {country} {year}\")\n",
    "    subset_df = gdf[(gdf['country'] == country) & (gdf['year'] == year)].reset_index(drop=True)\n",
    "    bboxes = {}\n",
    "    for _, item in subset_df.iterrows():\n",
    "        ymin, xmix, xmax, ymax = item.buffer.bounds\n",
    "        bboxes[item.id] = [ymin, xmix, xmax, ymax]\n",
    "    \n",
    "    get_pois(bboxes, year, country)\n",
    "    # print(f\"End {country} {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 123.02it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    tmp_df = pd.read_csv(f\"../../data/osm_features/{country}_{year}_pois.csv\")\n",
    "    total_df = total_df.append(tmp_df)\n",
    "total_df.to_csv(\"../../data/osm_features/_all_pois.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roads = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_filters = {\"residential\", \"track\", \"living_street\", \"trunk\", \"primary\", \"secondary\", \"tertiary\", \"service\", \"pedestrian\", \"intersection\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_func = client.elements.count.groupByBoundary.groupByTag\n",
    "len_func = client.elements.length.groupByBoundary.groupByTag\n",
    "dens_func = client.elements.length.density.groupByBoundary.groupByTag\n",
    "road_funcs = [count_func, len_func, dens_func]\n",
    "road_func_keys = [\"count\", \"length\", \"density\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_road_features(bboxes, year, country):\n",
    "    df_roads = pd.DataFrame()\n",
    "\n",
    "    response_count = client.elements.count.groupByBoundary.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\")\n",
    "    tota_count = process_response(response_count.as_dataframe(), \"total_count\")\n",
    "    df_roads = df_roads.append(tota_count)\n",
    "\n",
    "    response_len = client.elements.length.groupByBoundary.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\")\n",
    "    tota_len = process_response(response_len.as_dataframe(), \"total_length\")\n",
    "    df_roads = df_roads.merge(tota_len, on=\"id\")\n",
    "\n",
    "    response_dens = client.elements.length.density.groupByBoundary.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\")\n",
    "    tota_dens = process_response(response_dens.as_dataframe(), \"total_density\")\n",
    "    df_roads = df_roads.merge(tota_dens, on=\"id\")\n",
    "\n",
    "    road_dic = {}\n",
    "    for poi in road_filters:\n",
    "        road_dic[poi] = []\n",
    "\n",
    "    for key, func in zip(road_func_keys, road_funcs):\n",
    "        resp_road = func.post(bboxes=bboxes, time=f\"{year}-12-31\", filter=\"highway=* and type:way\", groupByKey=\"highway\").as_dataframe()\n",
    "        road_dic = {}\n",
    "        for poi in road_filters:\n",
    "            road_dic[f\"{key}_{poi}\"] = []\n",
    "\n",
    "        cur_key = resp_road.value.keys()[0][0]\n",
    "        cur_set = road_filters.copy()\n",
    "        values = resp_road.value.to_list()\n",
    "        road_dic[\"id\"] = [cur_key]\n",
    "        for i, cur in enumerate(resp_road.value.keys()):\n",
    "            # print(cur)\n",
    "            cur_poi = cur[1].replace(\"highway=\", \"\").replace(\" \", \"_\").lower()\n",
    "            if cur_poi in cur_set:\n",
    "                road_dic[f\"{key}_{cur_poi}\"].append(values[i])\n",
    "                cur_set.remove(cur_poi)\n",
    "\n",
    "            if cur[0] != cur_key:\n",
    "                cur_key = cur[0]\n",
    "                road_dic[\"id\"].append(cur_key)\n",
    "                for missing in cur_set:\n",
    "                    road_dic[f\"{key}_{missing}\"].append(0)\n",
    "                cur_set = road_filters.copy()\n",
    "                \n",
    "        for missing in cur_set:\n",
    "            road_dic[f\"{key}_{missing}\"].append(0)\n",
    "\n",
    "        tmp_dic = pd.DataFrame.from_dict(road_dic)\n",
    "        df_roads = df_roads.merge(tmp_dic, on=\"id\")\n",
    "    df_roads.to_csv(f\"../../data/osm_features/{country}_{year}_road.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 18624.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETH 2013\n",
      "ETH 2015\n",
      "ETH 2018\n",
      "MW 2016\n",
      "MW 2019\n",
      "NG 2012\n",
      "NG 2015\n",
      "NG 2018\n",
      "TZA 2012\n",
      "TZA 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "surveys = gdf.groupby([\"country\", \"year\"]).groups.keys()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    print(country, year)\n",
    "    if os.path.exists(f\"../../data/osm_features/{country}_{year}_road.csv\"):\n",
    "        continue\n",
    "    # print(f\"Start {country} {year}\")\n",
    "    subset_df = gdf[(gdf['country'] == country) & (gdf['year'] == year)].reset_index(drop=True)\n",
    "    bboxes = {}\n",
    "    for _, item in subset_df.iterrows():\n",
    "        ymin, xmix, xmax, ymax = item.buffer.bounds\n",
    "        bboxes[item.id] = [ymin, xmix, xmax, ymax]\n",
    "    \n",
    "    extract_road_features(bboxes, year, country)\n",
    "    # print(f\"End {country} {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 200.68it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame()\n",
    "for country, year in tqdm(surveys, total=len(surveys)):\n",
    "    tmp_df = pd.read_csv(f\"../../data/osm_features/{country}_{year}_road.csv\")\n",
    "    total_df = total_df.append(tmp_df)\n",
    "total_df.to_csv(\"../../data/osm_features/_all_road.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
