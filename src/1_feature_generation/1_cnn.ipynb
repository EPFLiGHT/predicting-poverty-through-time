{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral CNN\n",
    "\n",
    "This code is to train the Hyperspectral CNN. Warning: You need at least 18GB of RAM, to process the TfRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aamir/Documents/EPFL/MA 2/Semester Project/poverty-prediction-through-time/src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.tfrecordhelper import TfrecordHelper\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    \"\"\"\n",
    "    Helper to load dataset\n",
    "\n",
    "    Args:\n",
    "    - path (str): Path to dataset\n",
    "\n",
    "    Returns:\n",
    "    - dic which contains all data\n",
    "    \"\"\"\n",
    "    tf_helper = TfrecordHelper(path, ls_bands=\"ms\", nl_band=\"viirs\")\n",
    "    input_dic = {}\n",
    "    tf_helper.keyword_lat = \"lat\"\n",
    "    tf_helper.keyword_lon = \"lon\"\n",
    "    tf_helper.process_dataset()\n",
    "    for i, feature in enumerate(tf_helper.dataset):\n",
    "        input_dic[i] = {\n",
    "        \"year\": feature[\"years\"].numpy(),\n",
    "        \"cluster_lat\": feature[\"locs\"].numpy()[0],\n",
    "        \"cluster_lon\": feature[\"locs\"].numpy()[1],\n",
    "        \"img\": (feature[\"images\"][:,:,:7].numpy()),\n",
    "        \"nightlight\": np.mean(feature[\"images\"][:,:,7].numpy()),\n",
    "    }\n",
    "    \n",
    "    # Remove data where entry is broken (one channel contains only zeros)\n",
    "    remove = []\n",
    "    for feature in tqdm(input_dic):\n",
    "        if input_dic[feature][\"nightlight\"] == 0:\n",
    "            remove.append(feature)\n",
    "            continue\n",
    "        for dim in input_dic[feature][\"img\"]:\n",
    "            if not np.any(dim):\n",
    "                remove.append(feature)\n",
    "                break\n",
    "    \n",
    "    for r in remove:\n",
    "        input_dic.pop(r)\n",
    "    return input_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/tfrecords/raw/\"\n",
    "files = os.listdir(path) # path to the processed tfrecords from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dics = [] # will contain all information\n",
    "for file in files:\n",
    "    raw_path = \"path\" + file\n",
    "    data = load_dataset(raw_path)\n",
    "    input_dics.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[np.mean(X[:,i,:,:]) for i in range(7)], std=[np.std(X[:,i,:,:]) for i in range(7)])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "years = []\n",
    "lat = []\n",
    "lon = []\n",
    "for country in tqdm(input_dics):\n",
    "    data = country\n",
    "    for feature in data:\n",
    "        years.append(data[feature][\"year\"])\n",
    "        lat.append(data[feature][\"cluster_lat\"])\n",
    "        lon.append(data[feature][\"cluster_lon\"])\n",
    "        data[feature][\"img\"][:3,:,:] *=3 # RGB images to dark, got better performance by using it\n",
    "        X.append(data[feature][\"img\"])\n",
    "        y.append(data[feature][\"nightlight\"])\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(X[:,i,:,:]) for i in range(7)]\n",
    "stds = [np.std(X[:,i,:,:]) for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bins for nighttime images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightlights_to_class(data):\n",
    "    \"\"\"\n",
    "    Data are labels. Perform GMM based on the input and creates 5 classes out of it.\n",
    "\n",
    "    Args:\n",
    "    - data: radiance (nighttime images)\n",
    "\n",
    "    Return:\n",
    "    - list of labels\n",
    "    \"\"\"\n",
    "    x = data.reshape(-1,1)\n",
    "    gmm = GMM(n_components=5).fit(x)\n",
    "    labels = gmm.predict(x)\n",
    "    cut_label1 = data[labels==0].max()\n",
    "    cut_label2 = data[labels==1].max()\n",
    "    cut_label3 = data[labels==2].max()\n",
    "    cut_label4 = data[labels==3].max()\n",
    "    cut_label5 = data[labels==4].max()\n",
    "    cutoffs = [cut_label1, cut_label2, cut_label3,  cut_label4, cut_label5]\n",
    "    cutoffs = sorted(cutoffs)\n",
    "    \n",
    "    y_labels = []\n",
    "    for d in data:\n",
    "        if d <= cutoffs[0]:\n",
    "            y_labels.append(0)\n",
    "        elif d <= cutoffs[1]:\n",
    "            y_labels.append(1)\n",
    "        elif d <= cutoffs[2]:\n",
    "            y_labels.append(2)\n",
    "        elif d <= cutoffs[3]:\n",
    "            y_labels.append(3)\n",
    "        else:\n",
    "            y_labels.append(4)\n",
    "    return np.array(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = nightlights_to_class(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x.transpose(2, 0, 1)) # transpose is required by PyTorch\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(X, y_labels, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "split = int(np.floor(.4 * len(dataset)))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
    "                                                sampler=valid_sampler)\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": validation_loader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_sampler),\n",
    "    \"val\": len(valid_sampler)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) # load resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperspectral Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = nn.Conv2d(7, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), dilation=1, bias=False)\n",
    "model.conv1 = new_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_weights_all_countries_multichannel_{time.time()}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodel = torch.nn.Sequential(*list(model_ft.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    nmodel.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in input_dics:\n",
    "    for feature in tqdm(data, total=len(data)):\n",
    "        input_batch = preprocess(data[feature]['img'].transpose(2, 1, 0)).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = nmodel(input_batch.to('cuda'))\n",
    "        data[feature][\"feature\"] = np.squeeze(output.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge of weights and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for data in input_dics:\n",
    "    years = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    features = []\n",
    "    nightlights = []\n",
    "    for feature in tqdm(data, total=len(data)):\n",
    "        years.append(data[feature][\"year\"])\n",
    "        lat.append(data[feature][\"cluster_lat\"])\n",
    "        lon.append(data[feature][\"cluster_lon\"])\n",
    "        features.append(data[feature][\"feature\"].numpy().tolist())\n",
    "        nightlights.append(data[feature][\"nightlight\"])\n",
    "    tmp = pd.DataFrame.from_dict({\"year\": years, \"lat\": lat, 'lon': lon, \"features\": features, \"nightlight\": nightlights})\n",
    "    df = df.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/cnn_features/resnet_trans_all_countries_hyper.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
